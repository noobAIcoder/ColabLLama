{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/noobAIcoder/CollabLLama/blob/main/llama_cpp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Db87JWrVQyAn",
        "outputId": "18c31589-e718-4c24-c33d-3762e6e0f9ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'llama.cpp'...\n",
            "remote: Enumerating objects: 3435, done.\u001b[K\n",
            "remote: Counting objects: 100% (4/4), done.\u001b[K\n",
            "remote: Compressing objects: 100% (4/4), done.\u001b[K\n",
            "remote: Total 3435 (delta 0), reused 2 (delta 0), pack-reused 3431\u001b[K\n",
            "Receiving objects: 100% (3435/3435), 3.06 MiB | 19.08 MiB/s, done.\n",
            "Resolving deltas: 100% (2272/2272), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/ggerganov/llama.cpp.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd llama.cpp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_aVBxcFjRNWK",
        "outputId": "cbc81d4c-5085-4336-e4aa-fbbc8e170160"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/llama.cpp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!make LLAMA_CUBLAS=1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SSUk_OReRfUG",
        "outputId": "b1dc2d3e-979d-47c4-b555-1675a200bb0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I llama.cpp build info: \n",
            "I UNAME_S:  Linux\n",
            "I UNAME_P:  x86_64\n",
            "I UNAME_M:  x86_64\n",
            "I CFLAGS:   -I.              -O3 -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include\n",
            "I CXXFLAGS: -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include\n",
            "I LDFLAGS:   -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib\n",
            "I CC:       cc (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0\n",
            "I CXX:      g++ (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0\n",
            "\n",
            "cc  -I.              -O3 -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include   -c ggml.c -o ggml.o\n",
            "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -c llama.cpp -o llama.o\n",
            "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -c examples/common.cpp -o common.o\n",
            "cc -I.              -O3 -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include   -c -o k_quants.o k_quants.c\n",
            "nvcc --forward-unknown-to-host-compiler -arch=native -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_DMMV_Y=1 -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -Wno-pedantic -c ggml-cuda.cu -o ggml-cuda.o\n",
            "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include examples/main/main.cpp ggml.o llama.o common.o k_quants.o ggml-cuda.o -o main  -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib\n",
            "\n",
            "====  Run ./main -h for help.  ====\n",
            "\n",
            "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include examples/quantize/quantize.cpp ggml.o llama.o k_quants.o ggml-cuda.o -o quantize  -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib\n",
            "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include examples/quantize-stats/quantize-stats.cpp ggml.o llama.o k_quants.o ggml-cuda.o -o quantize-stats  -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib\n",
            "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include examples/perplexity/perplexity.cpp ggml.o llama.o common.o k_quants.o ggml-cuda.o -o perplexity  -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib\n",
            "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include examples/embedding/embedding.cpp ggml.o llama.o common.o k_quants.o ggml-cuda.o -o embedding  -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib\n",
            "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include pocs/vdot/vdot.cpp ggml.o k_quants.o ggml-cuda.o -o vdot  -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://huggingface.co/TheBloke/Wizard-Vicuna-7B-Uncensored-GGML/resolve/main/Wizard-Vicuna-7B-Uncensored.ggmlv3.q4_0.bin"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sbY-LpGoSHbi",
        "outputId": "e3d20fec-75ad-48bf-d17b-eef98d0adf5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-06-11 07:11:24--  https://huggingface.co/TheBloke/Wizard-Vicuna-7B-Uncensored-GGML/resolve/main/Wizard-Vicuna-7B-Uncensored.ggmlv3.q4_0.bin\n",
            "Resolving huggingface.co (huggingface.co)... 13.249.85.16, 13.249.85.92, 13.249.85.69, ...\n",
            "Connecting to huggingface.co (huggingface.co)|13.249.85.16|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs.huggingface.co/repos/f5/68/f5686f90902fee3f158a5955bb25137d1c05e56cec25fabc8481ceaba178668d/c31a4edd96527dcd808bcf9b99e3894065ac950747dac84ecd415a2387454e7c?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27Wizard-Vicuna-7B-Uncensored.ggmlv3.q4_0.bin%3B+filename%3D%22Wizard-Vicuna-7B-Uncensored.ggmlv3.q4_0.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1686726685&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2Y1LzY4L2Y1Njg2ZjkwOTAyZmVlM2YxNThhNTk1NWJiMjUxMzdkMWMwNWU1NmNlYzI1ZmFiYzg0ODFjZWFiYTE3ODY2OGQvYzMxYTRlZGQ5NjUyN2RjZDgwOGJjZjliOTllMzg5NDA2NWFjOTUwNzQ3ZGFjODRlY2Q0MTVhMjM4NzQ1NGU3Yz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODY3MjY2ODV9fX1dfQ__&Signature=LqLZNUoFEXHRS-ifm07qsfUL9-5Zu1b7SrdqEPbzzr0xEPoIGdNs-CYbzO3S3H2DmMgM2BfrM-Dj2pppxNThZZ5odsDzU4OxxEqw6l8lJugw6cNKcxDcBQcfEdnGRi7UDRAYA2unszo5an0A762jQN2C3xxSjKbBQL2Rk9wXXgF-mQmn%7E8FYitq0ZwHUF0x6zb9LAYB1Y0ew8fOAOwTSdBKiwHLYNdQvS%7EuGB%7EUlwF9ubykD7x3qYPpeQR%7Ek8YxnWHV2hyecKw9CFF6fwffBHRVePeQkG9lUd0kj4WmCJ7AlKtU0Yt1qma3e1zzq306q6Xj%7E2fcTkR7U4CqoFs3Jbw__&Key-Pair-Id=KVTP0A1DKRTAX [following]\n",
            "--2023-06-11 07:11:24--  https://cdn-lfs.huggingface.co/repos/f5/68/f5686f90902fee3f158a5955bb25137d1c05e56cec25fabc8481ceaba178668d/c31a4edd96527dcd808bcf9b99e3894065ac950747dac84ecd415a2387454e7c?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27Wizard-Vicuna-7B-Uncensored.ggmlv3.q4_0.bin%3B+filename%3D%22Wizard-Vicuna-7B-Uncensored.ggmlv3.q4_0.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1686726685&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2Y1LzY4L2Y1Njg2ZjkwOTAyZmVlM2YxNThhNTk1NWJiMjUxMzdkMWMwNWU1NmNlYzI1ZmFiYzg0ODFjZWFiYTE3ODY2OGQvYzMxYTRlZGQ5NjUyN2RjZDgwOGJjZjliOTllMzg5NDA2NWFjOTUwNzQ3ZGFjODRlY2Q0MTVhMjM4NzQ1NGU3Yz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODY3MjY2ODV9fX1dfQ__&Signature=LqLZNUoFEXHRS-ifm07qsfUL9-5Zu1b7SrdqEPbzzr0xEPoIGdNs-CYbzO3S3H2DmMgM2BfrM-Dj2pppxNThZZ5odsDzU4OxxEqw6l8lJugw6cNKcxDcBQcfEdnGRi7UDRAYA2unszo5an0A762jQN2C3xxSjKbBQL2Rk9wXXgF-mQmn%7E8FYitq0ZwHUF0x6zb9LAYB1Y0ew8fOAOwTSdBKiwHLYNdQvS%7EuGB%7EUlwF9ubykD7x3qYPpeQR%7Ek8YxnWHV2hyecKw9CFF6fwffBHRVePeQkG9lUd0kj4WmCJ7AlKtU0Yt1qma3e1zzq306q6Xj%7E2fcTkR7U4CqoFs3Jbw__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 13.249.85.11, 13.249.85.23, 13.249.85.116, ...\n",
            "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|13.249.85.11|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3791725184 (3.5G) [application/octet-stream]\n",
            "Saving to: ‘Wizard-Vicuna-7B-Uncensored.ggmlv3.q4_0.bin’\n",
            "\n",
            "Wizard-Vicuna-7B-Un 100%[===================>]   3.53G   207MB/s    in 24s     \n",
            "\n",
            "2023-06-11 07:11:49 (150 MB/s) - ‘Wizard-Vicuna-7B-Uncensored.ggmlv3.q4_0.bin’ saved [3791725184/3791725184]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./main -m Wizard-Vicuna-7B-Uncensored.ggmlv3.q4_0.bin -n 300 -p 'what is life?' -ngl 32"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dc0s5zHMTFuC",
        "outputId": "f9fa857c-9601-43f2-b372-fed8d59f58b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "main: build = 658 (4de0334)\n",
            "main: seed  = 1686467632\n",
            "ggml_init_cublas: found 1 CUDA devices:\n",
            "  Device 0: Tesla T4\n",
            "llama.cpp: loading model from Wizard-Vicuna-7B-Uncensored.ggmlv3.q4_0.bin\n",
            "llama_model_load_internal: format     = ggjt v3 (latest)\n",
            "llama_model_load_internal: n_vocab    = 32000\n",
            "llama_model_load_internal: n_ctx      = 512\n",
            "llama_model_load_internal: n_embd     = 4096\n",
            "llama_model_load_internal: n_mult     = 256\n",
            "llama_model_load_internal: n_head     = 32\n",
            "llama_model_load_internal: n_layer    = 32\n",
            "llama_model_load_internal: n_rot      = 128\n",
            "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
            "llama_model_load_internal: n_ff       = 11008\n",
            "llama_model_load_internal: n_parts    = 1\n",
            "llama_model_load_internal: model size = 7B\n",
            "llama_model_load_internal: ggml ctx size =    0.07 MB\n",
            "llama_model_load_internal: using CUDA for GPU acceleration\n",
            "llama_model_load_internal: mem required  = 1932.71 MB (+ 1026.00 MB per state)\n",
            "llama_model_load_internal: allocating batch_size x 1 MB = 512 MB VRAM for the scratch buffer\n",
            "llama_model_load_internal: offloading 32 layers to GPU\n",
            "llama_model_load_internal: total VRAM used: 3987 MB\n",
            "..................................................................................................\n",
            "llama_init_from_file: kv self size  =  256.00 MB\n",
            "\n",
            "system_info: n_threads = 2 / 2 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
            "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.800000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
            "generate: n_ctx = 512, n_batch = 512, n_predict = 300, n_keep = 0\n",
            "\n",
            "\n",
            " what is life?\n",
            "Life is a series of experiences that we have while we are on this earth. It can be anything from eating to having children, to getting married or even dying. Life is the sum total of everything that happens to us throughout our entire lives. It's the story of who we were, who we are and who we will become. [end of text]\n",
            "\n",
            "llama_print_timings:        load time =  3086.71 ms\n",
            "llama_print_timings:      sample time =    42.38 ms /    71 runs   (    0.60 ms per token)\n",
            "llama_print_timings: prompt eval time =   503.64 ms /     5 tokens (  100.73 ms per token)\n",
            "llama_print_timings:        eval time =  4861.60 ms /    70 runs   (   69.45 ms per token)\n",
            "llama_print_timings:       total time =  8011.84 ms\n"
          ]
        }
      ]
    }
  ]
}